{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #2 - Classification\n",
    "\n",
    "<font color=\"red\"> <b> Due: Feb 28 (Friday) 11:00 pm </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Jerry Valentine </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "\n",
    "The objective of this assignment is to get a better understanding of how machine learning models can work with classification data. In this assignemnt the pocket algorithm and the logistic regression algorithm will be compared with the Adult data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data\n",
    "\n",
    "For the classification data. I will be using adult income data, which can be found here: https://archive.ics.uci.edu/ml/datasets/Census+Income\n",
    "\n",
    "A quick overview of what will be done with the data is: reading the csv file, checking for null values, getting rid of null values and useless data, converting strings to integers, seperating the input data from the output data, plotting it as a heat map.\n",
    "\n",
    "Do note in observing the data I have used indicator values for each column just counting up from 0 depending on how many options it could have. When using the data with a model I have broken each of those out into its own respective column.\n",
    "\n",
    "This code cell is used to import the following packages:\n",
    "\n",
    "1. numpy - Used for its complex list manipulation and utility\n",
    "2. pandas - Used with numpy in order to add utilty to lists\n",
    "3. matplotlib - Used for plotting data\n",
    "\n",
    "This code cell also reads in the adult census data. The features of this data are:\n",
    "1. age - The age of the person\n",
    "2. workclass - The class of work the person does\n",
    "3. fnlwgt - How many people are predicted to be in this situation\n",
    "4. education - The highest level of education for this person\n",
    "5. education-num - A numerical value for the education\n",
    "6. marital-status - If the person is married, single or divorced.\n",
    "7. occupation - The type of job the person works\n",
    "8. relationship - The relationship this person has to others like wife or husband\n",
    "9. race - The race of the person\n",
    "10. sex - The sex of the person\n",
    "11. capital-gain - The average gain in capital over a year.\n",
    "12. capital-loss The average loss in capital over a year.\n",
    "13. hours-per-week - The number of hours worked a week.\n",
    "14. native-country - The country the person was born in.\n",
    "15. class - If the person earns over 50k a year or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"adult.csv\", delimiter=\", \", names=['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race', 'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'incomeclass'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell below is getting rid of any null values and checking to see if they are all gone. The educationnum feature is also dropped because I will convert the education column manualy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.workclass != '?']\n",
    "df = df[df.occupation != '?']\n",
    "df = df[df.nativecountry != '?']\n",
    "df = df.drop('educationnum', axis=1)\n",
    "\n",
    "np.any(df.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing all of that data I want to see the shape of the list to make sure there is still enough data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell converts every feature that has strings into numbers.\n",
    "Here are all of the mappings\n",
    "* workclass\n",
    "  0. Private\n",
    "  1. Self-emp-not-inc\n",
    "  2. Self-emp-inc\n",
    "  3. Federal-gov\n",
    "  4. Local-gov\n",
    "  5. State-gov\n",
    "  6. Without-pay\n",
    "  7. Never-worked\n",
    "* education\n",
    "  0. Preschool\n",
    "  1. 1st-4th\n",
    "  2. 5th-6th\n",
    "  3. 7th-8th\n",
    "  4. 9th\n",
    "  5. 10th\n",
    "  6. 11th\n",
    "  7. 12th\n",
    "  8. HS-grad\n",
    "  9. Some-college\n",
    "  10. Assoc-voc\n",
    "  11. Assoc-acdm\n",
    "  12. Prof-school\n",
    "  13. Bachelors\n",
    "  14. Masters\n",
    "  15. Doctorate\n",
    "* maritalstatus\n",
    "  0. Married-civ-spouse\n",
    "  1. Divorced\n",
    "  2. Never-married\n",
    "  3. Separated\n",
    "  4. Widowed\n",
    "  5. Married-spouse-absent\n",
    "  6. Married-AF-spouse\n",
    "* ocupation\n",
    "  0. Tech-support\n",
    "  1. Craft-repair\n",
    "  2. Other-service\n",
    "  3. Sales\n",
    "  4. Exec-managerial\n",
    "  5. Prof-specialty\n",
    "  6. Handlers-cleaners\n",
    "  7. Machine-op-inspct\n",
    "  8. Adm-clerical\n",
    "  9. Farming-fishing\n",
    "  10. Transport-moving\n",
    "  11. Priv-house-serv\n",
    "  12. Protective-serv\n",
    "  13. Armed-Forces\n",
    "* relationship\n",
    "  0. Wife\n",
    "  1. Own-child\n",
    "  2. Husband\n",
    "  3. Not-in-family\n",
    "  4. Other-relative\n",
    "  5. Unmarried\n",
    "* race\n",
    "  0. White\n",
    "  1. Asian-Pac-Islander\n",
    "  2. Amer-Indian-Eskimo\n",
    "  3. Other\n",
    "  4. Black\n",
    "* sex\n",
    "  0. Female\n",
    "  1. Male\n",
    "* nativecountry\n",
    "  0. United-States\n",
    "  1. Cambodia\n",
    "  2. England\n",
    "  3. Puerto-Rico\n",
    "  4. Canada\n",
    "  5. Germany\n",
    "  6. Outlying-US(Guam-USVI-etc)\n",
    "  7. India\n",
    "  8. Japan\n",
    "  9. Greece\n",
    "  10. South\n",
    "  11. China\n",
    "  12. Cuba\n",
    "  13. Iran\n",
    "  14. Honduras\n",
    "  15. Philippines\n",
    "  16. Italy\n",
    "  17. Poland\n",
    "  18. Jamaica\n",
    "  19. Vietnam\n",
    "  20. Mexico\n",
    "  21. Portugal\n",
    "  22. Ireland\n",
    "  23. France\n",
    "  24. Dominican-Republic\n",
    "  25. Laos\n",
    "  26. Ecuador\n",
    "  27. Taiwan\n",
    "  28. Haiti\n",
    "  29. Columbia\n",
    "  30. Hungary\n",
    "  31. Guatemala\n",
    "  32. Nicaragua\n",
    "  33. Scotland\n",
    "  34. Thailand\n",
    "  35. Yugoslavia\n",
    "  36. El-Salvador\n",
    "  37. Trinadad&Tobago\n",
    "  38. Peru\n",
    "  39. Hong\n",
    "  40. Holand-Netherlands\n",
    "* incomeclass\n",
    "  0. \\>50k\n",
    "  1. <=50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workclassdic = {'Private': 0, 'Self-emp-not-inc': 1, 'Self-emp-inc': 2, 'Federal-gov': 3, 'Local-gov': 4, 'State-gov': 5, 'Without-pay': 6, 'Never-worked': 7}\n",
    "educationdic = {'Preschool': 0, '1st-4th': 1, '5th-6th': 2, '7th-8th': 3, '9th': 4, '10th': 5, '11th': 6, '12th': 7, 'HS-grad': 8, 'Some-college': 9, 'Assoc-voc': 10, 'Assoc-acdm': 11, 'Prof-school': 12, 'Bachelors': 13, 'Masters': 14, 'Doctorate': 15}\n",
    "maritalstatusdic = {'Married-civ-spouse': 0, 'Divorced': 1, 'Never-married': 2, 'Separated': 3, 'Widowed': 4, 'Married-spouse-absent': 5, 'Married-AF-spouse': 6}\n",
    "occupationdic = {'Tech-support': 0, 'Craft-repair': 1, 'Other-service': 2, 'Sales': 3, 'Exec-managerial': 4, 'Prof-specialty': 5, 'Handlers-cleaners': 6, 'Machine-op-inspct': 7, 'Adm-clerical': 8, 'Farming-fishing': 9, 'Transport-moving': 10, 'Priv-house-serv': 11, 'Protective-serv': 12, 'Armed-Forces': 13}\n",
    "relationshipdic = {'Wife': 0, 'Own-child': 1, 'Husband': 2, 'Not-in-family': 3, 'Other-relative': 4, 'Unmarried': 5}\n",
    "racedic = {'White': 0, 'Asian-Pac-Islander': 1, 'Amer-Indian-Eskimo': 2, 'Other': 3, 'Black': 4}\n",
    "sexdic = {'Female': 0, 'Male': 1}\n",
    "nativecountrydic = {'United-States': 0, 'Cambodia': 1, 'England': 2, 'Puerto-Rico': 3, 'Canada': 4, 'Germany': 5, 'Outlying-US(Guam-USVI-etc)': 6, 'India': 7, 'Japan': 8, 'Greece': 9, 'South': 10, 'China': 11, 'Cuba': 12, 'Iran': 13, 'Honduras': 14, 'Philippines': 15, 'Italy': 16, 'Poland': 17, 'Jamaica': 18, 'Vietnam': 19, 'Mexico': 20, 'Portugal': 21, 'Ireland': 22, 'France': 23, 'Dominican-Republic': 24, 'Laos': 25, 'Ecuador': 26, 'Taiwan': 27, 'Haiti': 28, 'Columbia': 29, 'Hungary': 30, 'Guatemala': 31, 'Nicaragua': 32, 'Scotland': 33, 'Thailand': 34, 'Yugoslavia': 35, 'El-Salvador': 36, 'Trinadad&Tobago': 37, 'Peru': 38, 'Hong': 39, 'Holand-Netherlands': 40}\n",
    "incomeclassdic = {'>50K': 1, '<=50K': 0}\n",
    "\n",
    "df.workclass = df.workclass.apply(lambda s: workclassdic[s])\n",
    "df.education = df.education.apply(lambda s: educationdic[s])\n",
    "df.maritalstatus = df.maritalstatus.apply(lambda s: maritalstatusdic[s])\n",
    "df.occupation = df.occupation.apply(lambda s: occupationdic[s])\n",
    "df.relationship = df.relationship.apply(lambda s: relationshipdic[s])\n",
    "df.race = df.race.apply(lambda s: racedic[s])\n",
    "df.sex = df.sex.apply(lambda s: sexdic[s])\n",
    "df.nativecountry = df.nativecountry.apply(lambda s: nativecountrydic[s])\n",
    "df.incomeclass = df.incomeclass.apply(lambda s: incomeclassdic[s])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next two cells seperate the input data from the output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].copy()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = df.iloc[:, -1].copy()\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I've learned from preivous assignemtns is that a heat map has the best visual for this data set.[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(32,13))\n",
    "plt.clf() # Clear previous plt figure\n",
    "for i in range(13):\n",
    "    plt.subplot(3, 5, i+1) # Selects which subplot to plot to\n",
    "    plt.hist2d(X.iloc[:, i], T, bins=25, normed=False, cmap='plasma')\n",
    "    plt.ylabel(\"Income Class\") # Sets Y label\n",
    "    plt.xlabel(X.columns.values[i])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Observation\n",
    "\n",
    "From looking at the heat maps I can make a few observations. Very young people and very old people usally don't make over 50K a year. Mostly middle aged people. I think this makes sense becuase that is usally when you start to hit the peak of your career before you retire. Another observation I can make is any education higher than highschool has the best chance of making over 50K a year. The last observation I made was most people that make over 40K a year work 40 or more hours a week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Super Classs Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Super class for machine learning models \n",
    "\n",
    "class BaseModel(ABC):\n",
    "    \"\"\" Super class for ITCS Machine Learning Class\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(self, X, T):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def use(self, X):\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "class Classifier(BaseModel):\n",
    "    \"\"\"\n",
    "        Abstract class for classification \n",
    "        \n",
    "        Attributes\n",
    "        ==========\n",
    "        meanX       ndarray\n",
    "                    mean of inputs (from standardization)\n",
    "        stdX        ndarray\n",
    "                    standard deviation of inputs (standardization)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ):\n",
    "        self.meanX = None\n",
    "        self.stdX = None\n",
    "\n",
    "    def normalize(self, X):\n",
    "        \"\"\" standardize the input X \"\"\"\n",
    "        \n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.asanyarray(X)\n",
    "\n",
    "        # store the mean and std from the training set\n",
    "        # when you learned mean & std, we do not update for test\n",
    "        if self.meanX is None:\n",
    "            self.meanX = np.mean(X, 0)\n",
    "            self.stdX = np.std(X, 0)\n",
    "\n",
    "        Xs = copy(X)\n",
    "        Xs[:, 0] = (Xs[:, 0] - self.meanX[0]) / self.stdX[0] \n",
    "        Xs[:, 8] = (Xs[:, 8] - self.meanX[8]) / self.stdX[8] \n",
    "        Xs[:, 59:61] = (Xs[:, 59:61] - self.meanX[59:61]) / self.stdX[59:61] \n",
    "        return Xs\n",
    "\n",
    "    def _check_matrix(self, mat, name):\n",
    "        \"\"\" Utility to assure the input matrix mat is 2D. \n",
    "            If not, it throws an exception. \n",
    "            \n",
    "            mat     ndarray\n",
    "                    input matrix to check the shape\n",
    "            name    string\n",
    "                    matrix name to print out error\n",
    "        \"\"\"\n",
    "        if len(mat.shape) != 2:\n",
    "            raise ValueError(''.join([\"Wrong matrix \", name]))\n",
    "        \n",
    "    # add a basis\n",
    "    def add_ones(self, X):\n",
    "        \"\"\"\n",
    "            add a column basis to X input matrix\n",
    "        \"\"\"\n",
    "        self._check_matrix(X, 'X')\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "    ####################################################\n",
    "    #### abstract funcitons ############################\n",
    "    @abstractmethod\n",
    "    def train(self, X, T):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def use(self, X):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Pocket Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pocket(Classifier):\n",
    "    \n",
    "    def __init__(self, alpha):\n",
    "        Classifier.__init__(self)\n",
    "        self.alpha = alpha\n",
    "        self.w = np.random.rand(103)\n",
    "\n",
    "    # return 1 if w is better -1 if wp is better\n",
    "    def compare(self, X, T, wp):\n",
    "        y = np.sign(X @ self.w)\n",
    "        yp = np.sign(X @ wp)\n",
    "\n",
    "        return 1 if np.sum(y == T) >= np.sum(yp == T) else -1\n",
    "\n",
    "    def train(self, X, T):\n",
    "        maxiter = 10000\n",
    "\n",
    "        X1 = self.add_ones(X)\n",
    "        N = X1.shape[0]\n",
    "\n",
    "        w_pocket = copy(self.w)\n",
    "\n",
    "        plt.plot(T)\n",
    "        for i in range(maxiter):\n",
    "            print(i)\n",
    "            hasConverged = True\n",
    "            for k in np.random.permutation(N): #range(N):\n",
    "                y = self.w @ X1[k]\n",
    "                if np.sign(y) != np.sign(T[k]):\n",
    "                    self.w += self.alpha * T[k] * X1[k]\n",
    "                    hasConverged = False\n",
    "                    if self.compare(X1, T, w_pocket) > 0: \n",
    "                        w_pocket[:] = self.w[:]\n",
    "            \n",
    "            if hasConverged:\n",
    "                print(\"converged at \", i)\n",
    "                break\n",
    "\n",
    "        print(\"End of training: \", i)\n",
    "    \n",
    "    def use(self, X):\n",
    "        X1 = self.add_ones(X)\n",
    "        return X1 @ self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(Classifier):\n",
    "    \n",
    "    def __init__(self, alpha):\n",
    "        Classifier.__init__(self)\n",
    "        self.alpha = alpha\n",
    "        self.w = np.random.rand(103, 2)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        if not isinstance(z, np.ndarray):\n",
    "            z = np.asarray(z)\n",
    "        f = np.exp(z) \n",
    "        return f / (np.sum(f, axis=1, keepdims=True) if len(z.shape) == 2 else np.sum(f))\n",
    "\n",
    "    # for linear fx\n",
    "    def g(self, X):\n",
    "        return self.softmax(X @ self.w) \n",
    "\n",
    "    def train(self, X, T):\n",
    "        # iterate to update weights\n",
    "        niter = 10000\n",
    "        \n",
    "        X1 = self.add_ones(X)\n",
    "        for step in range(niter):\n",
    "            ys = self.g(X1)\n",
    "            # if(step == 0):\n",
    "            #     ys = np.reshape(ys, (ys.shape[0], 1))\n",
    "            self.w = self.w + self.alpha * X1.T @ (T - ys)\n",
    "    \n",
    "    def use(self, X):\n",
    "        X1 = self.add_ones(X)\n",
    "        return X1 @ self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition meathod used to partition the data into train and test [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" partitioning data\n",
    "\n",
    "    parameters\n",
    "    -----------\n",
    "    X        numpy array\n",
    "             input data to partition\n",
    "    T        numpy array\n",
    "             target labels to partition\n",
    "    raito    list\n",
    "             list of ratios for partitions (should be summed to 1) \n",
    "             the number of return pairs are different\n",
    "    return\n",
    "    -------\n",
    "    \n",
    "    Xs       list of numpy arrays\n",
    "    \n",
    "    Ts       list of numpy arrays\n",
    "\"\"\"\n",
    "def partition(X, T, ratio=[0.8, 0.2]): \n",
    "    \n",
    "    # Checks to make sure ratio sums to 1\n",
    "    assert(np.sum(ratio) == 1)\n",
    "    \n",
    "    # Store the number of data samples \n",
    "    N = X.shape[0]\n",
    "\n",
    "    # change the 1d array to 2d if need\n",
    "    if len(T.shape) == 1:\n",
    "        T = T.reshape((N,1))\n",
    "    \n",
    "    # Shuffle the data indices \n",
    "    idxs = np.random.permutation(N)\n",
    "        \n",
    "    Xs = []\n",
    "    Ts = []\n",
    "    i = 0  # first index to zero\n",
    "    for k, r in enumerate(ratio):\n",
    "        # Number of rows that corresponds to kth element in ratios\n",
    "        nrows = int(round(N * r)) \n",
    "        # If we are on the last ratio simply use the remaining data samples\n",
    "        if k == len(ratio)-1:\n",
    "            Xs.append(X[idxs[i:], :])\n",
    "            Ts.append(T[idxs[i:], :])\n",
    "        else:\n",
    "            Xs.append(X[idxs[i:i+nrows], :])\n",
    "            Ts.append(T[idxs[i:i+nrows], :])\n",
    "        \n",
    "        i += nrows\n",
    "    \n",
    "    return Xs, Ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Experiments\n",
    "\n",
    "Apply the classfiers on the data and discuss the results.\n",
    "Please describe your codes for experiments. You may have subsections of results and discussions here.\n",
    "Here follows the list that you consider to include:\n",
    "- the classification results\n",
    "- plots of classification results \n",
    "- model comparision \n",
    "- choice of evaluation metrics\n",
    "- **Must partition data into training and testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below will read in the data and display the adult data we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"adult.csv\", delimiter=\", \", names=['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race', 'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'incomeclass'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code will find any null values and get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.workclass != '?']\n",
    "df = df[df.occupation != '?']\n",
    "df = df[df.nativecountry != '?']\n",
    "df = df.drop('educationnum', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code block takes all of the columns that don't have numbers and blows them out into indicator columns. This creates saddly a bunch more columns then what my computer really likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicatorClass = pd.get_dummies(df.loc[:, 'workclass'])\n",
    "indicatorEducation = pd.get_dummies(df.loc[:, 'education'])\n",
    "indicatorMaritalstatus = pd.get_dummies(df.loc[:, 'maritalstatus'])\n",
    "indicatorOccupation = pd.get_dummies(df.loc[:, 'occupation'])\n",
    "indicatorRelationship = pd.get_dummies(df.loc[:, 'relationship'])\n",
    "indicatorRace = pd.get_dummies(df.loc[:, 'race'])\n",
    "indicatorSex = pd.get_dummies(df.loc[:, 'sex'])\n",
    "indicatorNativecountry = pd.get_dummies(df.loc[:, 'nativecountry'])\n",
    "\n",
    "incomeclassdic = {'>50K': 1, '<=50K': 0}\n",
    "df.incomeclass = df.incomeclass.apply(lambda s: incomeclassdic[s])\n",
    "\n",
    "df = pd.concat([\n",
    "        df.iloc[:, 0],\n",
    "        indicatorClass, \n",
    "        df.iloc[:, 2],\n",
    "        indicatorEducation,\n",
    "        indicatorMaritalstatus,\n",
    "        indicatorOccupation,\n",
    "        indicatorRelationship,\n",
    "        indicatorRace,\n",
    "        indicatorSex,\n",
    "        df.iloc[:, 9:11],\n",
    "        indicatorNativecountry,\n",
    "        df.iloc[:, -1:]],\n",
    "        axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the shape of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets take a look at the data's mean and standard deviation. From what we can see the data will need to be normalized in the columns that are not indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pocket Algorithm\n",
    "\n",
    "The first algorithm is the pocket algorithm. This uses the basic perceptron algorithm but insead of taking the last set of weights it remimbers which set of weights produced the best results and uses that one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block below creates the pocket algorithm object with an alpha of .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algoPocket = Pocket(.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the data is split up into input data and output data. Then the input data is normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1] \n",
    "T = df.iloc[:, -1]\n",
    "\n",
    "Xnorm = algoPocket.normalize(X)\n",
    "T = T.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is normalized we look at the mean and standard deviation again and it looks a lot better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Xnorm).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the data is split up into training and testing data. Here I also only choose the first 1000 data points for training and first 100 for testing becuase any more and the algorithm would take hours longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ts = partition(Xnorm, T)\n",
    "Xtrain, Xtest = Xs[0][:1000, :], Xs[1][:100, :]\n",
    "Ttrain, Ttest = Ts[0][0:1000], Ts[1][0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the pocket algorithm is being trained on out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titrain = (Ttrain == np.unique(Ttrain)).astype(int)\n",
    "Titest = (Ttest == np.unique(Ttest)).astype(int)\n",
    "algoPocket.train(Xtrain, Ttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the algorithm is tested out with the training and testing data. First lets look at the testing data. For the metric we will be looking at is accuracy. The accuracy for this isn't good at all. I think this has to do a lot with how many features this data now has and how little data we are using to train the model on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = algoPocket.use(Xtrain)\n",
    "\n",
    "plt.plot(Ttrain)\n",
    "plt.plot(Ytrain)\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy: \", 100 - np.mean(np.abs(Ttrain - Ytrain)) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing data doesn't look much better. It seems to have a lot of trouble predicting above 50k. I think this is because the data is skewed towards people having below 50k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytest = algoPocket.use(Xtest)\n",
    "\n",
    "plt.plot(Ttest)\n",
    "plt.plot(Ytest)\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy: \", 100 - np.mean(np.abs(Ttest - Ytest)) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The next model we will be comparing with the pocket algorithm is Logistic regression. This is a linear regression model that directly predicts P(T=kâˆ£x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best alpha I found for this model is .0000001. This first block just creates the Logistic Regression object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algoLogReg = LogisticRegression(.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block just trains the model on the data. We will be using the same data that has already been normalized and cut up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.argmax(Titrain, axis=1) == Ttrain.flatten()).all()\n",
    "assert (np.argmax(Titest, axis=1) == Ttest.flatten()).all()\n",
    "algoLogReg.train(Xtrain, Titrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will look at the training data and the accuracy it has. From what I can see the accuracy is pretty close to the number of data points that are below 50k. This tells me the model has a strong weight for guessing below 50k and for the most part only gets those guesses right. I still think this is due to not training on enough data for how many features are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = algoLogReg.use(Xtrain)\n",
    "\n",
    "Tl = np.argmax(Titrain, 1)\n",
    "Yl = np.argmax(Ytrain, 1)\n",
    "\n",
    "plt.plot(Tl, 'r')\n",
    "plt.plot(Yl, 'b')\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy: \", 100 - np.mean(np.abs(Tl - Yl)) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing data is all over the place it really just depends on how many below 50k data points are present. It mostly gets all of the below 50k right and struggles to get the above 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytest = algoLogReg.use(Xtest)\n",
    "\n",
    "Tl = np.argmax(Titest, 1)\n",
    "Yl = np.argmax(Ytest, 1)\n",
    "\n",
    "plt.plot(Tl, 'rx')\n",
    "plt.plot(Yl, 'bo')\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy: \", 100 - np.mean(np.abs(Tl - Yl)) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Overall I think the data I chose was just to much for me to properly proccess on my computer. I also think that this data doesn't fit a linear line well and will do better when using non-linear models with it. I do think the logistic regression preforms better with the data even though I think a linear model doesn't preform well on this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] \"4-Data Visualization.ipynb\", Minwoo Jake Lee, https://webpages.uncc.edu/mlee173/teach/itcs4156/progress.html<br>\n",
    "[2] \"matplotlib Heatmap\", matplotlib, \"https://riptutorial.com/matplotlib/example/17254/heatmap\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "\n",
    "* [OPT 1] Search for a ordinal data set and apply your classifiers to it. \n",
    "  - Repeat the experiments on it. \n",
    "  - Do you have different observation from previous results? \n",
    "  - Were you able to observe that we discussed in class about logistic regression? \n",
    "  - For a full extra credit point, you need to discuss all bullet points in Results section.     \n",
    "\n",
    "\n",
    "* [OPT 2] Partition your data into five sets. Selecting one test set and the other for training, repeat your experiments and observe/analyze the 5 different training/testing errors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "DO NOT forget to submit your data! Your notebook is supposed to run well after running your codes.\n",
    "\n",
    "To help our TA's grading, please make an explicit section for each grading criteria. \n",
    "\n",
    "** Note: this is a WRITING assignment. Proper writing is REQUIRED. Comments are not considered as writing. ** \n",
    "\n",
    "\n",
    "\n",
    "points | | description\n",
    "--|--|:--\n",
    "5 | Overview| states the objective and the appraoch \n",
    "15 | Data | \n",
    " | 5| description \n",
    " | 5| plots for understanding or analysis \n",
    " | 5| preliminary observation \n",
    "25 | Methods | \n",
    " |10| Summary of Classification models\n",
    " | 5| Explanation of codes\n",
    " |10| Pocket, Logistic Regression\n",
    "40 | Experiments \n",
    "| 5| Discussion about evaluation metrics\n",
    "| 5| Discussion about train and test accuracies\n",
    "|20| plots for results (10 for each algorithm)\n",
    "|10| Discussions about classificaion model comparison\n",
    "5 | |Conclusions \n",
    "5 | |Referemces\n",
    "5 | |Grammar and spelling error (Proofread please)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}